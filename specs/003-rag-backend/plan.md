# Implementation Plan: RAG Backend for Textbook Chatbot

**Branch**: `003-rag-backend` | **Date**: 2026-02-11 | **Spec**: [spec.md](./spec.md)
**Input**: Feature specification from `specs/003-rag-backend/spec.md`

## Summary

Build a FastAPI backend that powers a RAG chatbot for the Physical AI & Humanoid Robotics Handbook. The system ingests markdown lesson files using an adaptive chunking strategy (enrich short sections, preserve optimal sections, split long sections with overlap), stores embeddings in Qdrant Cloud via Cohere `embed-english-v3.0`, and serves search and chat endpoints. Chat answers are generated by Cohere Command-R with native document-grounded citations. The ingestion pipeline runs as a standalone CLI script, architecturally ready for GitHub webhook triggers.

## Technical Context

**Language/Version**: Python 3.10+
**Primary Dependencies**: FastAPI, Cohere SDK (v5+), qdrant-client (v1.12+), Pydantic v2, tiktoken, PyYAML
**Storage**: Qdrant Cloud (free tier, 1GB) — vector store; no relational DB for this phase
**Testing**: pytest + httpx (TestClient) for API integration tests, pytest for unit tests
**Target Platform**: Linux server (Railway), local development on Windows
**Project Type**: Single backend service (API + CLI ingestion)
**Performance Goals**: <3s end-to-end for chat (SC-001), <1s for search, 10 concurrent users (SC-006)
**Constraints**: Cohere free tier — 1,000 API calls/month total, 96 texts/batch embed, 5 embed calls/min, 20 chat calls/min
**Scale/Scope**: ~130-150 chunks for Module 1 (12 lessons, 128 H2 sections), scaling to ~500-800 for all 4 modules

## Constitution Check

*GATE: Must pass before Phase 0 research. Re-check after Phase 1 design.*

| Gate | Status | Evidence |
|------|--------|----------|
| Backend uses FastAPI (Python 3.10+) | PASS | Technical Context |
| Vector DB is Qdrant Cloud | PASS | Technical Context |
| Embeddings use Cohere (deviation from OpenAI in constitution) | PASS with NOTE | User explicitly requested Cohere; constitution update needed |
| Response format matches constitution JSON envelope | PASS | Contracts match `{success, data, error, meta}` pattern |
| CORS allows GitHub Pages + localhost | PASS | FR-014, config.py |
| Endpoint paths match `/api/*` convention | PASS | Contracts: `/api/health`, `/api/search`, `/api/chat` |
| Error codes match constitution taxonomy | PASS | `VALIDATION_ERROR`, `NOT_FOUND`, `RATE_LIMITED`, `SERVICE_UNAVAILABLE`, `INTERNAL_ERROR` |
| Python files use snake_case | PASS | Project structure |
| No secrets committed | PASS | `.env` + `.env.example` pattern |
| Auth deferred (not in this phase) | PASS | Spec Non-Goals section |

**Constitution deviation**: Embeddings changed from OpenAI to Cohere per user request. Update `.specify/memory/constitution.md` Tech Stack table after this feature merges.

## Project Structure

### Documentation (this feature)

```text
specs/003-rag-backend/
├── plan.md              # This file
├── spec.md              # Feature specification
├── research.md          # Phase 0: Technology research
├── data-model.md        # Phase 1: Entity definitions
├── quickstart.md        # Phase 1: Developer setup guide
├── contracts/
│   └── openapi.yaml     # Phase 1: API contract
└── tasks.md             # Phase 2: Task breakdown (created by /sp.tasks)
```

### Source Code (repository root)

```text
rag-backend/
├── app/
│   ├── __init__.py
│   ├── main.py                # FastAPI app: lifespan, CORS, router mounting
│   ├── config.py              # pydantic-settings: env vars → typed config
│   ├── models/
│   │   ├── __init__.py
│   │   ├── requests.py        # Pydantic: SearchRequest, ChatRequest
│   │   ├── responses.py       # Pydantic: ApiResponse[T], SearchResponse, ChatResponse, HealthResponse
│   │   └── domain.py          # Pydantic: Chunk, SectionType, LessonFrontmatter, IngestionReport
│   ├── api/
│   │   ├── __init__.py
│   │   ├── health.py          # GET /api/health → HealthResponse
│   │   ├── search.py          # POST /api/search → ApiResponse[SearchResponse]
│   │   └── chat.py            # POST /api/chat → ApiResponse[ChatResponse]
│   ├── services/
│   │   ├── __init__.py
│   │   ├── embeddings.py      # CohereEmbedService: batch embed with input_type
│   │   ├── vectorstore.py     # QdrantService: create, upsert, delete_by_lesson, search
│   │   ├── chat.py            # CohereChatService: Command-R with documents + citation mapping
│   │   ├── reranker.py        # CohereRerankService: rerank-v3.5 wrapper
│   │   └── query.py           # QueryExpander: structural section synonym expansion
│   ├── ingestion/
│   │   ├── __init__.py
│   │   ├── parser.py          # parse_lesson_file(): frontmatter + H2 section extraction
│   │   ├── chunker.py         # AdaptiveChunker: classify → enrich/preserve/split
│   │   └── pipeline.py        # IngestionPipeline: orchestrate parse → chunk → embed → upsert
│   └── ingest.py              # CLI entry: python -m app.ingest --docs-path <path>
├── tests/
│   ├── __init__.py
│   ├── conftest.py            # Shared fixtures: mock Cohere, mock Qdrant
│   ├── test_parser.py         # Frontmatter extraction, H2 splitting
│   ├── test_chunker.py        # Section classification, enrichment, splitting
│   ├── test_query.py          # Query expansion logic
│   ├── test_search.py         # Search endpoint integration
│   ├── test_chat.py           # Chat endpoint integration
│   └── test_health.py         # Health endpoint
├── .env.example               # Template with all required vars
├── requirements.txt           # Pinned production dependencies
├── requirements-dev.txt       # Test dependencies (pytest, httpx)
└── Procfile                   # Railway: web: uvicorn app.main:app --host 0.0.0.0 --port $PORT
```

**Structure Decision**: Single backend service with clear separation between API layer (`api/`), business logic (`services/`), and ingestion pipeline (`ingestion/`). The ingestion pipeline is importable as a module for CLI use and architecturally independent from the API server. Services are injected via FastAPI's dependency system using lifespan-scoped singletons.

## Architecture

### Request Flow: Search

```
Client → POST /api/search
  → SearchRequest (Pydantic validation)
  → QueryExpander.expand(query)                    # FR-018: structural expansion
  → CohereEmbedService.embed(query, "search_query") # 1 API call
  → QdrantService.search(vector, hardware_tier)     # Range(lte=tier)
  → CohereRerankService.rerank(query, results)      # 1 API call (optional)
  → ApiResponse[SearchResponse]
```

### Request Flow: Chat

```
Client → POST /api/chat
  → ChatRequest (Pydantic validation)
  → QueryExpander.expand(query)
  → CohereEmbedService.embed(query, "search_query") # 1 API call
  → QdrantService.search(vector, hardware_tier)
  → CohereRerankService.rerank(query, results)       # 1 API call (optional)
  → CohereChatService.generate(query, documents)     # 1 API call (Command-R)
    → maps Cohere citation objects → ChatSource[]
  → ApiResponse[ChatResponse]
```

### Ingestion Flow

```
CLI: python -m app.ingest --docs-path <path>
  → discover .md files in path
  → for each file:
      → parser.parse_lesson_file(path) → (LessonFrontmatter, List[H2Section])
      → chunker.create_chunks(sections, frontmatter) → List[Chunk]
        → classify section type (structural/instructional/code_heavy)
        → short (<200 tokens): enrich with lesson context
        → optimal (200-700 tokens): preserve as-is
        → long (>700 tokens): split with overlap, keep code blocks intact
      → QdrantService.delete_by_lesson(lesson_id)     # Remove old chunks
      → CohereEmbedService.batch_embed(chunks, "search_document")  # Batched (96/call)
      → QdrantService.upsert(chunks_with_vectors)     # Deterministic UUIDs
  → print IngestionReport
```

### Service Initialization (Lifespan)

```python
# app/main.py — services created once at startup, shared across requests
@asynccontextmanager
async def lifespan(app: FastAPI):
    settings = get_settings()
    cohere_client = cohere.AsyncClientV2(api_key=settings.cohere_api_key)
    qdrant_client = AsyncQdrantClient(url=settings.qdrant_url, api_key=settings.qdrant_api_key)

    app.state.embed_service = CohereEmbedService(cohere_client)
    app.state.vectorstore = QdrantService(qdrant_client, settings.collection_name)
    app.state.chat_service = CohereChatService(cohere_client)
    app.state.reranker = CohereRerankService(cohere_client)
    app.state.query_expander = QueryExpander()

    # Ensure collection exists
    await app.state.vectorstore.ensure_collection()

    yield

    await qdrant_client.close()
```

### Dependency Injection Pattern

```python
# app/api/search.py
from fastapi import APIRouter, Request

router = APIRouter()

@router.post("/api/search")
async def search(body: SearchRequest, request: Request):
    embed_service = request.app.state.embed_service
    vectorstore = request.app.state.vectorstore
    # ...
```

## Key Design Decisions

### 1. Deterministic Point IDs for Idempotent Re-ingestion

Point IDs are `uuid5(NAMESPACE_DNS, f"{lesson_id}:{section_title}:{chunk_index}")`. This means:
- Same content always produces the same ID
- Qdrant `upsert` overwrites existing points with matching IDs
- Combined with `delete_by_lesson(lesson_id)` before upsert, this guarantees zero duplicates (FR-009)

### 2. Cohere `input_type` Differentiation

Cohere v3 embeddings require `input_type` — using `"search_document"` for ingestion and `"search_query"` for queries. This is not optional; omitting it degrades retrieval quality significantly.

### 3. Adaptive Chunking Thresholds

Based on Module 1 analysis (128 sections):
- **<200 tokens** → enrich (38% of sections are structural, most under 241 tokens)
- **200-700 tokens** → preserve (covers 44% of sections: instructional + mixed)
- **>700 tokens** → split with 100-token overlap (18% are code-heavy, some reach 950 tokens)

### 4. Cohere Native Citations Over Custom Parsing

Cohere Command-R returns structured `citations` objects with character offsets and source references when you pass `documents`. This eliminates the need to build custom citation extraction. We map Cohere's source references back to our lesson metadata to produce `[Module X, Chapter Y, Lesson Z]` format.

### 5. Rerank as Optional Enhancement

Reranking improves precision but costs an API call. With 1,000 calls/month on free tier, we make rerank conditional:
- Skip if top vector search result has cosine score > 0.85
- Apply rerank only when results are ambiguous (top scores clustered)
- Can be disabled entirely via config flag

### 6. Auth-Ready Architecture

Services are injected via `app.state`, and endpoints receive the `Request` object. Adding auth middleware in a future phase means:
- Add a `get_current_user` dependency
- Inject it into endpoints that need auth
- Zero restructuring of service or business logic

## Complexity Tracking

No constitution violations requiring justification. All gates pass.

## Risk Assessment

| Risk | Impact | Mitigation |
|------|--------|------------|
| Cohere free tier exhausted (1,000 calls/month) | Chat/search stops working | Monitor call count; upgrade to production key before demo; batch ingestion aggressively |
| Qdrant Cloud free tier storage exceeded | Ingestion fails | ~150 chunks × 1024 floats × 4 bytes ≈ 0.6MB; well within 1GB limit |
| Variable chunk sizes degrade retrieval | Wrong results returned | Reranking compensates; query expansion for structural; integration test suite validates |
| Cohere API latency spikes | Exceeds 3s target (SC-001) | Async architecture; skip rerank if latency budget consumed; cache common queries |

## Follow-ups

1. Update constitution Tech Stack to replace OpenAI with Cohere
2. `/sp.tasks` to generate implementation task breakdown
3. After RAG is live: implement Better-Auth (next feature phase)
