---
id: lesson-4-sensor-systems
title: "Lesson 1.4: Sensor Systems"
sidebar_position: 4
sidebar_label: "1.4 Sensor Systems"
description: "Learn how LIDAR, depth cameras, IMUs, and force sensors enable robots to perceive and interact with the physical world"
duration_minutes: 60
proficiency_level: "A2"
layer: "L1"
hardware_tier: 1
tier_1_path: "Fully conceptual - no hardware needed"
learning_objectives:
  - "Define the four major sensor types used in humanoid robots"
  - "Explain how LIDAR measures distance using time-of-flight principles"
  - "Describe how IMUs enable balance and orientation tracking"
  - "Compare depth cameras and LIDAR for 3D perception tasks"
  - "Identify the role of force/torque sensors in manipulation"
keywords:
  - "LIDAR"
  - "depth camera"
  - "IMU"
  - "force sensor"
  - "perception"
  - "time-of-flight"
  - "sensor fusion"
  - "3D vision"
prerequisites:
  - "Lesson 1.3: The Humanoid Landscape"
chapter: "Chapter 1: Introduction to Physical AI"
module: "Module 1: The Robotic Nervous System â€” ROS 2"
---

# Lesson 1.4: Sensor Systems

**Duration**: 60 minutes
**Hardware Tier**: Tier 1 (Fully conceptual)
**Layer**: L1 (Manual Foundation)

## Learning Objectives

By the end of this lesson, you will be able to:
- Define the four major sensor types used in humanoid robots
- Explain how LIDAR measures distance using time-of-flight principles
- Describe how IMUs enable balance and orientation tracking
- Compare depth cameras and LIDAR for 3D perception tasks
- Identify the role of force/torque sensors in manipulation

## Why Robots Need Sensors

ChatGPT can write code, but it cannot see a coffee cup on a table. A humanoid robot needs sensors to perceive the physical world before it can act. Without sensors, a robot is blind, deaf, and numb. It cannot navigate around obstacles, maintain balance while walking, or grasp objects without crushing them. Sensors transform raw physical phenomena like light, motion, and pressure into digital data that the robot's brain can process. This lesson explores the four sensor types that give humanoid robots their perception capabilities.

## LIDAR: Measuring Distance with Light

**LIDAR** (Light Detection and Ranging) measures distance by sending out laser pulses and timing how long they take to bounce back. The sensor emits a beam of light, which travels to an object and reflects back to the sensor. By measuring the time-of-flight, LIDAR calculates the distance to that object. Modern LIDAR units spin or use mirrors to scan the environment, creating a 360-degree map of distances. Each measurement is called a range reading, and thousands of these readings combine to form a point cloud.

LIDAR excels at mapping and obstacle detection. Autonomous vehicles use LIDAR to detect pedestrians, other cars, and road boundaries. Humanoid robots use LIDAR to navigate indoor environments, avoiding furniture and walls. The technology works in complete darkness because it provides its own light source. However, LIDAR struggles with transparent surfaces like glass and highly reflective materials like mirrors. The laser beam either passes through or scatters unpredictably.

The range and accuracy of LIDAR vary by model. Low-cost units measure up to 10 meters with centimeter-level accuracy. High-end industrial LIDAR can reach 100 meters or more. For humanoid robots, a range of 10-20 meters is typically sufficient for indoor navigation. The sensor outputs data as an array of distance measurements, each corresponding to a specific angle. ROS 2 represents this data using the LaserScan message type, which includes the range values, angle increments, and timing information.

## Depth Cameras for 3D Vision

**Depth cameras** capture both color images and depth information for every pixel. Unlike regular cameras that only record color, depth cameras measure how far away each point in the scene is from the camera. The **Intel RealSense** series is a popular choice for robotics because it combines RGB color data with depth data in a single device. This combination allows robots to see objects in three dimensions, understanding not just what something looks like but also where it is in space.

Depth cameras use different technologies to measure distance. Stereo depth cameras use two lenses, like human eyes, to calculate depth through triangulation. Time-of-flight depth cameras emit infrared light and measure how long it takes to return, similar to LIDAR but across the entire image at once. Structured light cameras project a pattern of dots or lines onto the scene and analyze how the pattern deforms to calculate depth. Each approach has trade-offs in range, accuracy, and computational cost.

The advantage of depth cameras over LIDAR is their ability to provide rich visual information. A robot can use the color image to identify objects using computer vision, then use the depth data to determine how far away those objects are. This enables tasks like picking up a specific item from a cluttered table. Depth cameras typically have a shorter range than LIDAR, usually 0.5 to 10 meters, but they provide much denser spatial information. The output is a depth image where each pixel contains a distance value, often called a depth map or point cloud.

## IMUs: The Robot's Inner Ear

An **IMU** (Inertial Measurement Unit) measures acceleration and rotation, giving the robot a sense of its own motion and orientation. Just as your inner ear helps you maintain balance, an IMU helps a robot know which way is up and whether it is tilting or falling. The sensor combines an accelerometer, which measures linear acceleration in three axes, and a gyroscope, which measures rotational velocity around three axes. Some IMUs also include a magnetometer to measure the Earth's magnetic field for compass-like heading information.

IMUs are critical for humanoid robots because walking requires constant balance adjustments. When a robot lifts one foot, its center of mass shifts, and the IMU detects this change. The robot's control system uses this information to adjust the position of its other leg and torso to prevent falling. IMUs operate at high frequencies, often 100 to 1000 times per second, providing real-time feedback for balance control. Without an IMU, a humanoid robot would have no sense of whether it is upright or toppling over.

The challenge with IMUs is drift. Accelerometers and gyroscopes measure changes in motion, not absolute position. Over time, small measurement errors accumulate, causing the estimated position and orientation to drift away from the true values. This is why robots combine IMU data with other sensors like cameras or LIDAR in a process called sensor fusion. The IMU provides fast, high-frequency updates, while other sensors provide occasional corrections to prevent drift. Modern robotics frameworks like ROS 2 include tools for fusing IMU data with other sensor inputs to produce accurate state estimates.

## Force and Torque Sensors for Touch

**Force sensors** measure how much push or pull is applied to them, while **torque sensors** measure rotational force. These sensors give robots a sense of touch, allowing them to interact gently with objects and people. When a humanoid robot grasps a cup, force sensors in the fingers detect how hard the robot is squeezing. If the force is too low, the cup will slip. If the force is too high, the cup will break. The robot adjusts its grip based on this feedback.

Force and torque sensors are often placed at robot joints and in the hands or feet. In the joints, they measure the forces acting on the robot's limbs, which helps with balance and collision detection. If a robot's arm bumps into an obstacle, the force sensor detects the unexpected resistance, and the robot can stop or change direction. In the hands, force sensors enable delicate manipulation tasks like threading a needle or shaking hands with a person without causing injury.

These sensors typically use strain gauges, which are materials that change electrical resistance when deformed. When force is applied, the sensor element bends slightly, changing its resistance. Electronics measure this change and convert it to a force reading. Multi-axis force/torque sensors can measure forces and torques in all three dimensions simultaneously, providing complete information about the interaction. This data is essential for tasks that require compliance, where the robot must adapt its motion based on the forces it encounters rather than following a rigid pre-programmed path.

## How Sensors Enable the Perception-Action Loop

Sensors are the input side of the **perception-action loop**, the fundamental cycle that drives all robot behavior. The loop begins with sensors gathering data about the environment and the robot's own state. This raw sensor data flows into perception algorithms that interpret the data, identifying objects, estimating positions, and detecting events. The robot's decision-making system uses this interpreted information to choose actions. Actuators execute these actions, changing the robot's state or the environment. Sensors then measure the results, and the loop continues.

Each sensor type contributes different information to this loop. LIDAR provides spatial awareness for navigation. Depth cameras enable object recognition and manipulation. IMUs supply balance and motion feedback. Force sensors give tactile information for interaction. No single sensor can provide all the information a robot needs. Modern humanoid robots use sensor fusion, combining data from multiple sensors to build a complete picture of the world.

The timing of this loop is critical. Sensors must update fast enough for the robot to react to changes. IMUs update at hundreds of hertz for balance control. LIDAR and cameras typically update at 10 to 30 hertz for navigation and perception. Force sensors update at high rates for manipulation tasks. ROS 2 provides the infrastructure to manage these different update rates, ensuring that each part of the robot receives the sensor data it needs when it needs it. Understanding how sensors feed the perception-action loop is the foundation for building intelligent robot behaviors.

## Key Takeaways

- LIDAR measures distance using time-of-flight, emitting laser pulses and timing their return to create 360-degree maps of the environment.
- Depth cameras like Intel RealSense combine color images with depth information, enabling robots to see objects in three dimensions.
- IMUs measure acceleration and rotation, providing the high-frequency feedback needed for balance and motion control in humanoid robots.
- Force and torque sensors give robots a sense of touch, enabling gentle manipulation and collision detection.
- Sensor fusion combines data from multiple sensor types to overcome individual sensor limitations and build a complete perception system.

## Check Your Understanding

1. Explain why LIDAR cannot reliably detect glass walls or mirrors. What property of these materials causes the problem?

2. A humanoid robot needs to pick up a fragile egg from a table. Which two sensor types would be most important for this task, and what role would each play?

3. Compare the strengths and weaknesses of LIDAR versus depth cameras for indoor navigation. In what situations would you choose one over the other?

4. Why do IMUs experience drift over time, and how do robots compensate for this problem?

5. Describe the perception-action loop and identify where each of the four sensor types (LIDAR, depth camera, IMU, force sensor) contributes to the loop.

## Next Steps

Now that you understand how sensors give robots perception, the next chapter will explore ROS 2, the software framework that connects sensors to robot brains. You will learn how ROS 2 organizes sensor data into messages and distributes them to the programs that need them.

---

**Hardware Tier 1 Note**: This lesson is fully conceptual and requires no hardware. When you progress to later modules with hands-on sensor work, Tier 1 students can use simulated sensors in cloud-based environments like Google Colab with Gazebo simulation, while Tier 2+ students can work with physical sensors.
