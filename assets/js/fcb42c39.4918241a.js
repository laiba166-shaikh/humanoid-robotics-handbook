"use strict";(globalThis.webpackChunkhumanoid_textbook=globalThis.webpackChunkhumanoid_textbook||[]).push([[581],{3145(e,n,o){o.r(n),o.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"module-4-vla/module-4-vla","title":"Module 4: Vision-Language-Action (VLA)","description":"The convergence of LLMs and robotics - voice commands to robot actions","source":"@site/docs/module-4-vla/README.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/","permalink":"/docs/module-4-vla/","draft":false,"unlisted":false,"editUrl":"https://github.com/laiba166-shaikh/humanoid-robotics-handbook/tree/main/humanoid-textbook/docs/module-4-vla/README.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"id":"module-4-vla","title":"Module 4: Vision-Language-Action (VLA)","sidebar_position":5,"sidebar_label":"Module 4: VLA","description":"The convergence of LLMs and robotics - voice commands to robot actions"},"sidebar":"tutorialSidebar","previous":{"title":"Ch 3: Deployment","permalink":"/docs/module-3-isaac/chapter-3-deployment/"},"next":{"title":"Ch 1: Humanoid Dev","permalink":"/docs/module-4-vla/chapter-1-humanoid-dev/"}}');var t=o(4848),s=o(8453);const r={id:"module-4-vla",title:"Module 4: Vision-Language-Action (VLA)",sidebar_position:5,sidebar_label:"Module 4: VLA",description:"The convergence of LLMs and robotics - voice commands to robot actions"},l="Module 4: Vision-Language-Action (VLA)",a={},d=[{value:"Module Overview",id:"module-overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Hardware Requirements",id:"hardware-requirements",level:2},{value:"Chapters",id:"chapters",level:2},{value:"Chapter 1: Humanoid Robot Development",id:"chapter-1-humanoid-robot-development",level:3},{value:"Chapter 2: Conversational Robotics",id:"chapter-2-conversational-robotics",level:3},{value:"Chapter 3: Capstone Project",id:"chapter-3-capstone-project",level:3},{value:"Capstone Project: The Autonomous Humanoid",id:"capstone-project-the-autonomous-humanoid",level:2}];function c(e){const n={a:"a",admonition:"admonition",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Focus"}),": The convergence of LLMs and Robotics\r\n",(0,t.jsx)(n.strong,{children:"Duration"}),": Weeks 11-13 (3 weeks)\r\n",(0,t.jsx)(n.strong,{children:"Hardware Tier"}),": Tier 2-4"]}),"\n",(0,t.jsx)(n.admonition,{title:"Coming Soon",type:"note",children:(0,t.jsx)(n.p,{children:"This module is currently in outline form. Full content will be available in a future update."})}),"\n",(0,t.jsx)(n.h2,{id:"module-overview",children:"Module Overview"}),"\n",(0,t.jsx)(n.p,{children:"This is where everything comes together. Vision-Language-Action (VLA) models represent the cutting edge of robotics: robots that can see, understand natural language, and act in the physical world."}),"\n",(0,t.jsx)(n.p,{children:'Tell your robot "Clean the room" and watch it plan a path, navigate obstacles, identify objects, and manipulate them - all from a single voice command.'}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this module, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Integrate"})," OpenAI Whisper for voice-to-text commands"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Design"})," cognitive planning systems using LLMs"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Build"})," multi-modal interaction (speech, gesture, vision)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Create"})," action sequences from natural language"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Complete"})," the Autonomous Humanoid capstone project"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Modules 1-3 completed"}),"\n",(0,t.jsx)(n.li,{children:"Understanding of LLMs and prompt engineering"}),"\n",(0,t.jsx)(n.li,{children:"Python async programming"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"hardware-requirements",children:"Hardware Requirements"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Tier"}),(0,t.jsx)(n.th,{children:"Equipment"}),(0,t.jsx)(n.th,{children:"What You Can Do"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Tier 2"})}),(0,t.jsx)(n.td,{children:"RTX GPU"}),(0,t.jsx)(n.td,{children:"Full VLA pipeline in simulation"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Tier 3"})}),(0,t.jsx)(n.td,{children:"Jetson + Sensors"}),(0,t.jsx)(n.td,{children:"Real-world voice commands"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Tier 4"})}),(0,t.jsx)(n.td,{children:"Physical Robot"}),(0,t.jsx)(n.td,{children:"Complete autonomous humanoid"})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"chapters",children:"Chapters"}),"\n",(0,t.jsx)(n.h3,{id:"chapter-1-humanoid-robot-development",children:"Chapter 1: Humanoid Robot Development"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.em,{children:"Weeks 11-12 \u2022 4 Lessons"})}),"\n",(0,t.jsx)(n.p,{children:"Kinematics, locomotion, and manipulation for humanoid robots."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"/docs/module-4-vla/chapter-1-humanoid-dev/",children:"View Chapter Outline \u2192"})}),"\n",(0,t.jsx)(n.h3,{id:"chapter-2-conversational-robotics",children:"Chapter 2: Conversational Robotics"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.em,{children:"Week 13 \u2022 4 Lessons"})}),"\n",(0,t.jsx)(n.p,{children:"Voice-to-action, speech recognition, and cognitive planning with LLMs."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"/docs/module-4-vla/chapter-2-conversational/",children:"View Chapter Outline \u2192"})}),"\n",(0,t.jsx)(n.h3,{id:"chapter-3-capstone-project",children:"Chapter 3: Capstone Project"}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.em,{children:"Final Week \u2022 3 Lessons"})}),"\n",(0,t.jsx)(n.p,{children:"The Autonomous Humanoid - your culminating project."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.a,{href:"/docs/module-4-vla/chapter-3-capstone/",children:"View Chapter Outline \u2192"})}),"\n",(0,t.jsx)(n.h2,{id:"capstone-project-the-autonomous-humanoid",children:"Capstone Project: The Autonomous Humanoid"}),"\n",(0,t.jsx)(n.p,{children:"Build a simulated humanoid robot that:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Receives"}),' a voice command ("Pick up the red cup")']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Plans"})," a sequence of actions using an LLM"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Navigates"})," to the target location avoiding obstacles"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Identifies"})," the target object using computer vision"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Manipulates"})," the object (grasping, moving)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Reports"})," completion back to the user"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This is Physical AI in action - the future of human-robot collaboration."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453(e,n,o){o.d(n,{R:()=>r,x:()=>l});var i=o(6540);const t={},s=i.createContext(t);function r(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);