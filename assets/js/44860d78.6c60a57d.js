"use strict";(globalThis.webpackChunkhumanoid_textbook=globalThis.webpackChunkhumanoid_textbook||[]).push([[808],{1448(e,n,i){i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-4-vla/chapter-2-conversational/chapter-2-conversational","title":"Chapter 2: Conversational Robotics","description":"Voice-to-action, speech recognition, and cognitive planning with LLMs","source":"@site/docs/module-4-vla/chapter-2-conversational/README.md","sourceDirName":"module-4-vla/chapter-2-conversational","slug":"/module-4-vla/chapter-2-conversational/","permalink":"/humanoid-robotics-handbook/docs/module-4-vla/chapter-2-conversational/","draft":false,"unlisted":false,"editUrl":"https://github.com/laiba166-shaikh/humanoid-robotics-handbook/tree/main/humanoid-textbook/docs/module-4-vla/chapter-2-conversational/README.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"id":"chapter-2-conversational","title":"Chapter 2: Conversational Robotics","sidebar_position":2,"sidebar_label":"Ch 2: Conversational","description":"Voice-to-action, speech recognition, and cognitive planning with LLMs"},"sidebar":"tutorialSidebar","previous":{"title":"Ch 1: Humanoid Dev","permalink":"/humanoid-robotics-handbook/docs/module-4-vla/chapter-1-humanoid-dev/"},"next":{"title":"Ch 3: Capstone","permalink":"/humanoid-robotics-handbook/docs/module-4-vla/chapter-3-capstone/"}}');var o=i(4848),s=i(8453);const r={id:"chapter-2-conversational",title:"Chapter 2: Conversational Robotics",sidebar_position:2,sidebar_label:"Ch 2: Conversational",description:"Voice-to-action, speech recognition, and cognitive planning with LLMs"},a="Chapter 2: Conversational Robotics",l={},c=[{value:"Chapter Overview",id:"chapter-overview",level:2},{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Lessons (Outline)",id:"lessons-outline",level:2},{value:"Key Topics Covered",id:"key-topics-covered",level:2}];function d(e){const n={admonition:"admonition",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"chapter-2-conversational-robotics",children:"Chapter 2: Conversational Robotics"})}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Duration"}),": Week 13\r\n",(0,o.jsx)(n.strong,{children:"Hardware Tier"}),": Tier 2-3\r\n",(0,o.jsx)(n.strong,{children:"Lessons"}),": 4"]}),"\n",(0,o.jsx)(n.admonition,{title:"Coming Soon",type:"note",children:(0,o.jsx)(n.p,{children:"This chapter is currently in outline form. Full content will be available in a future update."})}),"\n",(0,o.jsx)(n.h2,{id:"chapter-overview",children:"Chapter Overview"}),"\n",(0,o.jsx)(n.p,{children:'"Clean the room." Four syllables that require understanding intent, decomposing into subtasks, perceiving the environment, and executing dozens of coordinated actions. This chapter teaches you how to build robots that understand and act on natural language.'}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Integrate"})," OpenAI Whisper for voice command processing"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Build"})," speech recognition and NLU pipelines"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Design"})," multi-modal interaction systems"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Use"})," LLMs for cognitive planning and task decomposition"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"lessons-outline",children:"Lessons (Outline)"}),"\n",(0,o.jsxs)(n.table,{children:[(0,o.jsx)(n.thead,{children:(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.th,{children:"#"}),(0,o.jsx)(n.th,{children:"Lesson"}),(0,o.jsx)(n.th,{children:"Duration"}),(0,o.jsx)(n.th,{children:"Status"})]})}),(0,o.jsxs)(n.tbody,{children:[(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"2.1"}),(0,o.jsx)(n.td,{children:"Voice-to-Action with OpenAI Whisper"}),(0,o.jsx)(n.td,{children:"75 min"}),(0,o.jsx)(n.td,{children:"\ud83d\udcdd Outline"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"2.2"}),(0,o.jsx)(n.td,{children:"Speech Recognition and NLU"}),(0,o.jsx)(n.td,{children:"60 min"}),(0,o.jsx)(n.td,{children:"\ud83d\udcdd Outline"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"2.3"}),(0,o.jsx)(n.td,{children:"Multi-Modal Interaction"}),(0,o.jsx)(n.td,{children:"75 min"}),(0,o.jsx)(n.td,{children:"\ud83d\udcdd Outline"})]}),(0,o.jsxs)(n.tr,{children:[(0,o.jsx)(n.td,{children:"2.4"}),(0,o.jsx)(n.td,{children:"Cognitive Planning with LLMs"}),(0,o.jsx)(n.td,{children:"90 min"}),(0,o.jsx)(n.td,{children:"\ud83d\udcdd Outline"})]})]})]}),"\n",(0,o.jsx)(n.h2,{id:"key-topics-covered",children:"Key Topics Covered"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Whisper integration with ROS 2"}),"\n",(0,o.jsx)(n.li,{children:"Intent classification and entity extraction"}),"\n",(0,o.jsx)(n.li,{children:"Gesture recognition with MediaPipe"}),"\n",(0,o.jsx)(n.li,{children:"LLM-based task decomposition"}),"\n",(0,o.jsx)(n.li,{children:"Action primitive libraries"}),"\n",(0,o.jsx)(n.li,{children:"Grounding language in robot capabilities"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453(e,n,i){i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const o={},s=t.createContext(o);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);