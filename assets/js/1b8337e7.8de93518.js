"use strict";(globalThis.webpackChunkhumanoid_textbook=globalThis.webpackChunkhumanoid_textbook||[]).push([[691],{8747(e,n,s){s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>i,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"module-1-ros2/chapter-3-building-ros2/lesson-2-rclpy-agents","title":"Lesson 3.2: Bridging Python Agents with rclpy","description":"Learn to integrate Python AI models and agents with ROS 2 using rclpy patterns for intelligent robot control","source":"@site/docs/module-1-ros2/chapter-3-building-ros2/lesson-2-rclpy-agents.md","sourceDirName":"module-1-ros2/chapter-3-building-ros2","slug":"/module-1-ros2/chapter-3-building-ros2/lesson-2-rclpy-agents","permalink":"/docs/module-1-ros2/chapter-3-building-ros2/lesson-2-rclpy-agents","draft":false,"unlisted":false,"editUrl":"https://github.com/laiba166-shaikh/humanoid-robotics-handbook/tree/main/humanoid-textbook/docs/module-1-ros2/chapter-3-building-ros2/lesson-2-rclpy-agents.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"id":"lesson-2-rclpy-agents","title":"Lesson 3.2: Bridging Python Agents with rclpy","sidebar_position":2,"sidebar_label":"3.2 Python AI Agents","description":"Learn to integrate Python AI models and agents with ROS 2 using rclpy patterns for intelligent robot control","duration_minutes":75,"proficiency_level":"B1","layer":"L2","hardware_tier":2,"tier_1_path":"Use cloud-based Jupyter notebooks to prototype AI models, then examine integration patterns conceptually","learning_objectives":["Integrate Python AI models with ROS 2 nodes using rclpy","Design perception-decision-action loops for autonomous behavior","Implement state management for AI agents in long-running nodes","Apply callback patterns for real-time sensor data processing","Build AI-driven control nodes that publish commands based on sensor input"],"keywords":["rclpy patterns","AI integration","Python agents","perception action loop","machine learning ROS 2","autonomous control"],"prerequisites":["Lesson 3.1: Building ROS 2 Packages with Python","Python intermediate (classes, async concepts)","Basic machine learning concepts (models, inference)"],"chapter":"Chapter 3: Building with ROS 2","module":"Module 1: The Robotic Nervous System \u2014 ROS 2"},"sidebar":"tutorialSidebar","previous":{"title":"3.1 ROS 2 Packages","permalink":"/docs/module-1-ros2/chapter-3-building-ros2/lesson-1-packages-python"},"next":{"title":"3.3 Launch & Parameters","permalink":"/docs/module-1-ros2/chapter-3-building-ros2/lesson-3-launch-params"}}');var t=s(4848),o=s(8453);const i={id:"lesson-2-rclpy-agents",title:"Lesson 3.2: Bridging Python Agents with rclpy",sidebar_position:2,sidebar_label:"3.2 Python AI Agents",description:"Learn to integrate Python AI models and agents with ROS 2 using rclpy patterns for intelligent robot control",duration_minutes:75,proficiency_level:"B1",layer:"L2",hardware_tier:2,tier_1_path:"Use cloud-based Jupyter notebooks to prototype AI models, then examine integration patterns conceptually",learning_objectives:["Integrate Python AI models with ROS 2 nodes using rclpy","Design perception-decision-action loops for autonomous behavior","Implement state management for AI agents in long-running nodes","Apply callback patterns for real-time sensor data processing","Build AI-driven control nodes that publish commands based on sensor input"],keywords:["rclpy patterns","AI integration","Python agents","perception action loop","machine learning ROS 2","autonomous control"],prerequisites:["Lesson 3.1: Building ROS 2 Packages with Python","Python intermediate (classes, async concepts)","Basic machine learning concepts (models, inference)"],chapter:"Chapter 3: Building with ROS 2",module:"Module 1: The Robotic Nervous System \u2014 ROS 2"},a="Lesson 3.2: Bridging Python Agents with rclpy",l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Why AI Integration Matters in Robotics",id:"why-ai-integration-matters-in-robotics",level:2},{value:"The Perception-Decision-Action Pattern",id:"the-perception-decision-action-pattern",level:2},{value:"Structuring an AI Agent Node",id:"structuring-an-ai-agent-node",level:2},{value:"Integrating Machine Learning Models",id:"integrating-machine-learning-models",level:2},{value:"Managing Agent State Across Callbacks",id:"managing-agent-state-across-callbacks",level:2},{value:"Performance Considerations for Real-Time Control",id:"performance-considerations-for-real-time-control",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Check Your Understanding",id:"check-your-understanding",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={admonition:"admonition",code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"lesson-32-bridging-python-agents-with-rclpy",children:"Lesson 3.2: Bridging Python Agents with rclpy"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Duration"}),": 75 minutes\r\n",(0,t.jsx)(n.strong,{children:"Hardware Tier"}),": Tier 2 (RTX GPU + Ubuntu)\r\n",(0,t.jsx)(n.strong,{children:"Layer"}),": L2 (AI Collaboration)"]}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Integrate Python AI models with ROS 2 nodes using rclpy"}),"\n",(0,t.jsx)(n.li,{children:"Design perception-decision-action loops for autonomous behavior"}),"\n",(0,t.jsx)(n.li,{children:"Implement state management for AI agents in long-running nodes"}),"\n",(0,t.jsx)(n.li,{children:"Apply callback patterns for real-time sensor data processing"}),"\n",(0,t.jsx)(n.li,{children:"Build AI-driven control nodes that publish commands based on sensor input"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"why-ai-integration-matters-in-robotics",children:"Why AI Integration Matters in Robotics"}),"\n",(0,t.jsx)(n.p,{children:"A warehouse robot needs to navigate around obstacles while optimizing its path for efficiency. The navigation algorithm uses a neural network trained on thousands of warehouse layouts. How do you connect this Python AI model to the robot's sensors and motors? This lesson teaches you to bridge the gap between AI code and ROS 2 control systems, enabling intelligent autonomous behavior."}),"\n",(0,t.jsx)(n.h2,{id:"the-perception-decision-action-pattern",children:"The Perception-Decision-Action Pattern"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.strong,{children:"perception-decision-action loop"})," is the fundamental pattern for autonomous robots. The robot perceives its environment through sensors, an AI agent makes decisions based on that data, and the robot acts by sending commands to actuators. This loop runs continuously, allowing the robot to respond to changing conditions."]}),"\n",(0,t.jsx)(n.p,{children:"In ROS 2, this pattern maps naturally to the publisher-subscriber model. Sensor nodes publish perception data to topics. Your AI agent node subscribes to these topics, processes the data through a model, and publishes action commands to control topics. The beauty of this architecture is that you can swap AI models without changing the sensor or actuator nodes."}),"\n",(0,t.jsxs)(n.p,{children:["Consider a collision avoidance system. A LIDAR sensor publishes distance measurements to ",(0,t.jsx)(n.code,{children:"/scan"}),". Your AI node subscribes to ",(0,t.jsx)(n.code,{children:"/scan"}),", feeds the data into a neural network that predicts safe velocities, and publishes the result to ",(0,t.jsx)(n.code,{children:"/cmd_vel"}),". The motor controller subscribes to ",(0,t.jsx)(n.code,{children:"/cmd_vel"})," and moves the robot. Each component is independent and testable."]}),"\n",(0,t.jsx)(n.p,{children:"The key challenge is timing. Sensors publish data at fixed rates, but AI inference takes variable time depending on model complexity. You must design your node to handle this asynchrony. ROS 2 callbacks help by processing messages as they arrive, but you need to ensure your AI model runs fast enough to keep up with sensor rates. For real-time control, aim for inference times under 100 milliseconds."}),"\n",(0,t.jsx)(n.h2,{id:"structuring-an-ai-agent-node",children:"Structuring an AI Agent Node"}),"\n",(0,t.jsx)(n.p,{children:"An AI agent node combines standard ROS 2 patterns with AI model management. The node initializes the model once during startup, then uses it repeatedly in callbacks. This structure keeps inference fast and memory efficient."}),"\n",(0,t.jsx)(n.p,{children:"The typical structure includes three components: model initialization, sensor callbacks, and command publishing. During initialization, you load your trained model and set up publishers and subscribers. In callbacks, you preprocess sensor data, run inference, and publish results. This separation of concerns makes your code maintainable and testable."}),"\n",(0,t.jsx)(n.p,{children:"Here is the basic pattern for an AI agent node:"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"What we're building"}),": An obstacle avoidance node that uses a simple decision model to control robot velocity based on LIDAR data."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# obstacle_avoidance_agent.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import LaserScan\r\nfrom geometry_msgs.msg import Twist\r\nimport numpy as np\r\n\r\nclass ObstacleAvoidanceAgent(Node):\r\n    def __init__(self):\r\n        super().__init__('obstacle_avoidance_agent')\r\n\r\n        # Initialize AI model (simple rule-based for demonstration)\r\n        self.min_safe_distance = 0.5  # meters\r\n        self.max_speed = 0.5  # m/s\r\n\r\n        # Set up subscribers and publishers\r\n        self.scan_sub = self.create_subscription(\r\n            LaserScan,\r\n            '/scan',\r\n            self.scan_callback,\r\n            10\r\n        )\r\n        self.cmd_pub = self.create_publisher(Twist, '/cmd_vel', 10)\r\n\r\n        self.get_logger().info('Obstacle avoidance agent initialized')\r\n\r\n    def scan_callback(self, msg):\r\n        # Perception: Extract relevant sensor data\r\n        front_distances = msg.ranges[len(msg.ranges)//4:3*len(msg.ranges)//4]\r\n        min_distance = min(front_distances)\r\n\r\n        # Decision: Apply AI model (simple threshold logic)\r\n        velocity = self.compute_safe_velocity(min_distance)\r\n\r\n        # Action: Publish control command\r\n        cmd = Twist()\r\n        cmd.linear.x = velocity\r\n        self.cmd_pub.publish(cmd)\r\n\r\n        self.get_logger().info(f'Distance: {min_distance:.2f}m, Velocity: {velocity:.2f}m/s')\r\n\r\n    def compute_safe_velocity(self, distance):\r\n        \"\"\"AI decision model: scale velocity based on obstacle distance\"\"\"\r\n        if distance < self.min_safe_distance:\r\n            return 0.0  # Stop\r\n        elif distance < self.min_safe_distance * 2:\r\n            return self.max_speed * 0.3  # Slow down\r\n        else:\r\n            return self.max_speed  # Full speed\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = ObstacleAvoidanceAgent()\r\n    rclpy.spin(node)\r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Expected output"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"[INFO] [obstacle_avoidance_agent]: Obstacle avoidance agent initialized\r\n[INFO] [obstacle_avoidance_agent]: Distance: 2.34m, Velocity: 0.50m/s\r\n[INFO] [obstacle_avoidance_agent]: Distance: 0.87m, Velocity: 0.15m/s\r\n[INFO] [obstacle_avoidance_agent]: Distance: 0.42m, Velocity: 0.00m/s\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"What's happening"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Lines 12-14: Model parameters are initialized once during node construction"}),"\n",(0,t.jsx)(n.li,{children:"Lines 16-22: Subscriber receives LIDAR data, publisher sends velocity commands"}),"\n",(0,t.jsx)(n.li,{children:"Lines 25-37: Callback implements the perception-decision-action loop"}),"\n",(0,t.jsx)(n.li,{children:"Lines 39-46: Decision model is separated into its own method for clarity and testing"}),"\n",(0,t.jsx)(n.li,{children:"The node processes each LIDAR scan and immediately publishes a velocity command"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["This structure scales to complex AI models. Replace ",(0,t.jsx)(n.code,{children:"compute_safe_velocity()"})," with a neural network inference call, and the rest of the code remains the same."]}),"\n",(0,t.jsx)(n.h2,{id:"integrating-machine-learning-models",children:"Integrating Machine Learning Models"}),"\n",(0,t.jsx)(n.p,{children:"Real AI agents use trained models like neural networks or decision trees. Python's rich ML ecosystem makes integration straightforward. You load the model during node initialization and call it in callbacks."}),"\n",(0,t.jsx)(n.p,{children:"The key consideration is model format. PyTorch and TensorFlow models need their respective libraries available in your ROS 2 environment. ONNX provides a framework-agnostic format that works across platforms. For production systems, consider converting models to ONNX for better portability."}),"\n",(0,t.jsx)(n.admonition,{title:"ML Model Safety",type:"danger",children:(0,t.jsx)(n.p,{children:"Always validate ML model outputs before sending commands to physical robots. Implement velocity limits, emergency stops, and sensor validation. Test ML controllers extensively in simulation before deploying to hardware. Never trust model predictions without bounds checking."})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"What we're building"}),": A node that uses a scikit-learn model to predict optimal robot speed based on sensor features."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# ml_velocity_controller.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import LaserScan\r\nfrom geometry_msgs.msg import Twist\r\nimport numpy as np\r\nfrom sklearn.ensemble import RandomForestRegressor\r\nimport pickle\r\n\r\nclass MLVelocityController(Node):\r\n    def __init__(self):\r\n        super().__init__('ml_velocity_controller')\r\n\r\n        # Load pre-trained model\r\n        try:\r\n            with open('velocity_model.pkl', 'rb') as f:\r\n                self.model = pickle.load(f)\r\n            self.get_logger().info('ML model loaded successfully')\r\n        except FileNotFoundError:\r\n            self.get_logger().error('Model file not found, using default behavior')\r\n            self.model = None\r\n\r\n        # Set up ROS 2 communication\r\n        self.scan_sub = self.create_subscription(\r\n            LaserScan, '/scan', self.scan_callback, 10\r\n        )\r\n        self.cmd_pub = self.create_publisher(Twist, '/cmd_vel', 10)\r\n\r\n    def extract_features(self, scan_msg):\r\n        \"\"\"Convert LIDAR scan to feature vector for ML model\"\"\"\r\n        ranges = np.array(scan_msg.ranges)\r\n\r\n        # Create features: min, mean, std of distances in different sectors\r\n        front = ranges[len(ranges)//3:2*len(ranges)//3]\r\n        left = ranges[:len(ranges)//3]\r\n        right = ranges[2*len(ranges)//3:]\r\n\r\n        features = np.array([\r\n            np.min(front), np.mean(front), np.std(front),\r\n            np.min(left), np.min(right)\r\n        ])\r\n        return features.reshape(1, -1)\r\n\r\n    def scan_callback(self, msg):\r\n        if self.model is None:\r\n            return\r\n\r\n        # Extract features from sensor data\r\n        features = self.extract_features(msg)\r\n\r\n        # Run ML inference\r\n        predicted_velocity = self.model.predict(features)[0]\r\n        predicted_velocity = np.clip(predicted_velocity, 0.0, 0.5)\r\n\r\n        # Publish command\r\n        cmd = Twist()\r\n        cmd.linear.x = float(predicted_velocity)\r\n        self.cmd_pub.publish(cmd)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = MLVelocityController()\r\n    rclpy.spin(node)\r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == '__main__':\r\n    main()\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Expected output"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"[INFO] [ml_velocity_controller]: ML model loaded successfully\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"What's happening"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Lines 15-20: Model is loaded from disk during initialization, with error handling"}),"\n",(0,t.jsx)(n.li,{children:"Lines 29-42: Feature extraction converts raw sensor data into model input format"}),"\n",(0,t.jsx)(n.li,{children:"Lines 49-51: ML inference predicts velocity, with clipping to ensure safe bounds"}),"\n",(0,t.jsx)(n.li,{children:"Line 55: Predicted velocity is published as a Twist message"}),"\n",(0,t.jsx)(n.li,{children:"The model runs on every LIDAR scan, providing continuous control"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["This pattern works for any scikit-learn model. For deep learning, replace pickle with PyTorch's ",(0,t.jsx)(n.code,{children:"torch.load()"})," or TensorFlow's ",(0,t.jsx)(n.code,{children:"tf.keras.models.load_model()"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"managing-agent-state-across-callbacks",children:"Managing Agent State Across Callbacks"}),"\n",(0,t.jsx)(n.p,{children:"Many AI agents need to maintain state between callbacks. A path planning agent remembers the current goal. A learning agent tracks performance metrics. ROS 2 nodes are perfect for this because they persist between callbacks."}),"\n",(0,t.jsxs)(n.p,{children:["Store state as instance variables in your node class. Initialize state in ",(0,t.jsx)(n.code,{children:"__init__()"}),", update it in callbacks, and use it in decision-making. This approach keeps state management explicit and testable."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"What we're building"}),": A goal-seeking agent that remembers its target and adjusts behavior based on progress."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# goal_seeking_agent.py\r\nimport rclpy\r\nfrom rclpy.node import Node\r\nfrom geometry_msgs.msg import Twist, PoseStamped\r\nfrom nav_msgs.msg import Odometry\r\nimport math\r\n\r\nclass GoalSeekingAgent(Node):\r\n    def __init__(self):\r\n        super().__init__(\'goal_seeking_agent\')\r\n\r\n        # Agent state\r\n        self.current_pose = None\r\n        self.goal_pose = None\r\n        self.goal_reached = False\r\n        self.distance_threshold = 0.5  # meters\r\n\r\n        # ROS 2 communication\r\n        self.odom_sub = self.create_subscription(\r\n            Odometry, \'/odom\', self.odom_callback, 10\r\n        )\r\n        self.goal_sub = self.create_subscription(\r\n            PoseStamped, \'/goal_pose\', self.goal_callback, 10\r\n        )\r\n        self.cmd_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\r\n\r\n        # Control timer\r\n        self.timer = self.create_timer(0.1, self.control_loop)\r\n\r\n        self.get_logger().info(\'Goal-seeking agent ready\')\r\n\r\n    def odom_callback(self, msg):\r\n        """Update current position from odometry"""\r\n        self.current_pose = msg.pose.pose\r\n\r\n    def goal_callback(self, msg):\r\n        """Receive new goal from user or planner"""\r\n        self.goal_pose = msg.pose\r\n        self.goal_reached = False\r\n        self.get_logger().info(f\'New goal received: ({msg.pose.position.x:.2f}, {msg.pose.position.y:.2f})\')\r\n\r\n    def control_loop(self):\r\n        """Main decision loop running at 10 Hz"""\r\n        if self.current_pose is None or self.goal_pose is None:\r\n            return\r\n\r\n        if self.goal_reached:\r\n            return\r\n\r\n        # Calculate distance to goal\r\n        dx = self.goal_pose.position.x - self.current_pose.position.x\r\n        dy = self.goal_pose.position.y - self.current_pose.position.y\r\n        distance = math.sqrt(dx**2 + dy**2)\r\n\r\n        # Check if goal reached\r\n        if distance < self.distance_threshold:\r\n            self.goal_reached = True\r\n            self.stop_robot()\r\n            self.get_logger().info(\'Goal reached!\')\r\n            return\r\n\r\n        # Compute control command\r\n        angle_to_goal = math.atan2(dy, dx)\r\n        cmd = Twist()\r\n        cmd.linear.x = min(0.3, distance * 0.5)  # Proportional control\r\n        cmd.angular.z = angle_to_goal * 0.5\r\n        self.cmd_pub.publish(cmd)\r\n\r\n    def stop_robot(self):\r\n        """Send zero velocity command"""\r\n        cmd = Twist()\r\n        self.cmd_pub.publish(cmd)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = GoalSeekingAgent()\r\n    rclpy.spin(node)\r\n    node.destroy_node()\r\n    rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Expected output"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"[INFO] [goal_seeking_agent]: Goal-seeking agent ready\r\n[INFO] [goal_seeking_agent]: New goal received: (5.00, 3.00)\r\n[INFO] [goal_seeking_agent]: Goal reached!\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"What's happening"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Lines 12-16: State variables track current position, goal, and completion status"}),"\n",(0,t.jsx)(n.li,{children:"Lines 32-40: Callbacks update state when new data arrives"}),"\n",(0,t.jsx)(n.li,{children:"Lines 42-66: Control loop uses state to make decisions at fixed rate"}),"\n",(0,t.jsx)(n.li,{children:"Lines 54-59: State determines whether to continue or stop"}),"\n",(0,t.jsx)(n.li,{children:"The agent maintains context across multiple sensor readings and commands"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This pattern enables complex behaviors like finite state machines, learning algorithms, and multi-step plans."}),"\n",(0,t.jsx)(n.h2,{id:"performance-considerations-for-real-time-control",children:"Performance Considerations for Real-Time Control"}),"\n",(0,t.jsx)(n.p,{children:"AI models must run fast enough for real-time control. A robot moving at 1 meter per second travels 10 centimeters in 100 milliseconds. If your model takes longer than that to compute, the robot reacts too slowly."}),"\n",(0,t.jsxs)(n.p,{children:["Profile your model inference time before deploying. Use Python's ",(0,t.jsx)(n.code,{children:"time"})," module to measure how long callbacks take. For deep learning models, consider using GPU acceleration with CUDA. For simpler models, ensure you are using optimized libraries like NumPy with BLAS support."]}),"\n",(0,t.jsx)(n.p,{children:"If your model is too slow, you have several options. First, simplify the model architecture or reduce input dimensions. Second, run inference at a lower rate than sensor updates, using the most recent prediction until a new one is ready. Third, offload inference to a separate thread or process to avoid blocking callbacks. ROS 2 supports multi-threaded executors for this purpose."}),"\n",(0,t.jsx)(n.p,{children:"Another consideration is memory usage. Loading large models consumes RAM. If you run multiple AI nodes, memory can become a bottleneck. Consider sharing models across nodes using ROS 2 services, where one node hosts the model and others request predictions. This pattern reduces memory usage and centralizes model updates."}),"\n",(0,t.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"The perception-decision-action loop maps naturally to ROS 2's publisher-subscriber pattern, with AI agents subscribing to sensors and publishing to actuators."}),"\n",(0,t.jsx)(n.li,{children:"AI agent nodes load models during initialization and call them in callbacks, keeping inference fast and memory efficient."}),"\n",(0,t.jsx)(n.li,{children:"Machine learning models integrate easily with rclpy using standard Python libraries like scikit-learn, PyTorch, or TensorFlow."}),"\n",(0,t.jsx)(n.li,{children:"Agent state persists as instance variables in the node class, enabling complex behaviors that span multiple callbacks."}),"\n",(0,t.jsx)(n.li,{children:"Real-time control requires model inference times under 100 milliseconds, achievable through model optimization, GPU acceleration, or asynchronous execution."}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"check-your-understanding",children:"Check Your Understanding"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Describe the three stages of the perception-decision-action loop and how they map to ROS 2 publishers and subscribers."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"You have a PyTorch model that takes 200 milliseconds to run inference, but your robot needs control updates every 50 milliseconds. What are three strategies to handle this timing mismatch?"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Examine this code: ",(0,t.jsx)(n.code,{children:"self.model = pickle.load(f)"})," in ",(0,t.jsx)(n.code,{children:"__init__()"}),". Why is it better to load the model during initialization rather than in the callback function?"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Your AI agent needs to remember the last 10 sensor readings to detect trends. How would you implement this state management in a ROS 2 node?"}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"You want to run the same object detection model in three different nodes (navigation, manipulation, and logging). What ROS 2 pattern would you use to avoid loading the model three times?"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"Now that you can integrate AI agents with ROS 2, the next lesson covers launch files and parameters. You will learn to start multiple nodes together, configure them with parameters, and manage complex multi-node systems."}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Hardware Tier 1 Note"}),": Use cloud-based Jupyter notebooks or Google Colab to prototype and test your AI models. Examine the integration patterns in this lesson conceptually, and prepare to implement them when you have access to a local ROS 2 environment. The AI model development can happen entirely in the cloud, with only the ROS 2 integration requiring Tier 2 hardware."]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453(e,n,s){s.d(n,{R:()=>i,x:()=>a});var r=s(6540);const t={},o=r.createContext(t);function i(e){const n=r.useContext(o);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);